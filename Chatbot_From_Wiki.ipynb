{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot_From_Wiki.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyvyMjsC+BDbnunMXQBquV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mithunkumarsr/NLPNov21/blob/main/Chatbot_From_Wiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8XroZ7MFjk-"
      },
      "source": [
        "https://stackabuse.com/python-for-nlp-creating-a-rule-based-chatbot/\n",
        "\n",
        "\n",
        "The chatbot we are going to develop will be very simple. First we need a corpus that contains lots of information about the sport of tennis. We will develop such a corpus by scraping the Wikipedia article on tennis. Next, we will perform some preprocessing on the corpus and then will divide the corpus into sentences.\n",
        "\n",
        "When a user enters a query, the query will be converted into vectorized form. All the sentences in the corpus will also be converted into their corresponding vectorized forms. Next, the sentence with the highest cosine similarity with the user input vector will be selected as a response to the user input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkUhM2AjB1me"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVlEWdMSCDog"
      },
      "source": [
        "raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Tennis')\n",
        "raw_html = raw_html.read()\n",
        "\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for para in article_paragraphs:\n",
        "    article_text += para.text\n",
        "\n",
        "article_text = article_text.lower()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgvVqCOSDleK"
      },
      "source": [
        "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
        "article_text = re.sub(r'\\s+', ' ', article_text)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwHhCXXoDqpT",
        "outputId": "ad737105-4845-4154-ac3f-44e0cdf46f56"
      },
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "article_sentences = nltk.sent_tokenize(article_text)\n",
        "article_words = nltk.word_tokenize(article_text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDme0x1XD_Jj"
      },
      "source": [
        "wnlemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def perform_lemmatization(tokens):\n",
        "    return [wnlemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
        "\n",
        "def get_processed_text(document):\n",
        "    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB_KxxrqECtL"
      },
      "source": [
        "greeting_inputs = (\"hey\", \"good morning\", \"good evening\", \"morning\", \"evening\", \"hi\", \"whatsup\")\n",
        "greeting_responses = [\"hey\", \"hey hows you?\", \"*nods*\", \"hello, how you doing\", \"hello\", \"Welcome, I am good and you\"]\n",
        "\n",
        "def generate_greeting_response(greeting):\n",
        "    for token in greeting.split():\n",
        "        if token.lower() in greeting_inputs:\n",
        "            return random.choice(greeting_responses)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7KswGTiEFvX"
      },
      "source": [
        "Responding to User Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sMYMud4EGxt"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGw6_SzhELl2"
      },
      "source": [
        "def generate_response(user_input):\n",
        "    tennisrobo_response = ''\n",
        "    article_sentences.append(user_input)\n",
        "\n",
        "    word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n",
        "    all_word_vectors = word_vectorizer.fit_transform(article_sentences)\n",
        "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
        "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
        "\n",
        "    matched_vector = similar_vector_values.flatten()\n",
        "    matched_vector.sort()\n",
        "    vector_matched = matched_vector[-2]\n",
        "\n",
        "    if vector_matched == 0:\n",
        "        tennisrobo_response = tennisrobo_response + \"I am sorry, I could not understand you\"\n",
        "        return tennisrobo_response\n",
        "    else:\n",
        "        tennisrobo_response = tennisrobo_response + article_sentences[similar_sentence_number]\n",
        "        return tennisrobo_response"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVxTg5CeEORP",
        "outputId": "0c1e3549-a93f-41ab-867f-274c1148bc34"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n",
        "all_word_vectors = word_vectorizer.fit_transform(article_sentences)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rKb7V7xEbhx"
      },
      "source": [
        "similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1gMQliKEeGz"
      },
      "source": [
        "similar_sentence_number = similar_vector_values.argsort()[0][-2]\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcuK32dzEgyu"
      },
      "source": [
        "Chatting with the Chatbot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmVd7peUEhfu",
        "outputId": "ebf829ad-eb2c-4ca5-cb2b-d6fdc02e5262"
      },
      "source": [
        "continue_dialogue = True\n",
        "print(\"Hello, I am your friend TennisRobo. You can ask me any question regarding tennis:\")\n",
        "while(continue_dialogue == True):\n",
        "    human_text = input()\n",
        "    human_text = human_text.lower()\n",
        "    if human_text != 'bye':\n",
        "        if human_text == 'thanks' or human_text == 'thank you very much' or human_text == 'thank you':\n",
        "            continue_dialogue = False\n",
        "            print(\"TennisRobo: Most welcome\")\n",
        "        else:\n",
        "            if generate_greeting_response(human_text) != None:\n",
        "                print(\"TennisRobo: \" + generate_greeting_response(human_text))\n",
        "            else:\n",
        "                print(\"TennisRobo: \", end=\"\")\n",
        "                print(generate_response(human_text))\n",
        "                article_sentences.remove(human_text)\n",
        "    else:\n",
        "        continue_dialogue = False\n",
        "        print(\"TennisRobo: Good bye and take care of yourself...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, I am your friend TennisRobo. You can ask me any question regarding tennis:\n",
            "which year was rules formed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TennisRobo: there were different rules at each club.\n",
            "contact\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TennisRobo: when the contact point is higher than the reach of a two-handed backhand, most players will try to execute a high slice (under the ball or sideways).\n",
            "right hand\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TennisRobo: two hands give the player more control, while one hand can generate a slice shot, applying backspin on the ball to produce a low trajectory bounce.\n"
          ]
        }
      ]
    }
  ]
}